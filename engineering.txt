https://github.com/njd87/cs2620_clock

03/02/2025

(Did via VS Code Liveshare in Dhall, so little commit history. Unit tests added by mention.)

Starting work today on the logical clocks. Before we begin, we have an outline of roughly what we want to complete:

1) Each vm will be an object of a VirtualMachine class that keeps track of the other clocks
    Also useful since we can set clock speed at the beginning!
2) Listening sockets need to be run within each vm for each socket This is a bit tough
since we also do not have a main "server," but rather computers talking to each other over distinct sockets (maybe using one socket per listening?)
3) When clock speed is chosen, we need to sleep for time/clock seconds for each event.
4) From what we have decided, vector clocks are NOT mentioned. So, internal events update clock by 1 and when it receives another clock's time,
new time is max(our_time, their_time) + 1 as described in lecture.
5) Logging would also be best by type of message sent/received. I.e., having [INTERNAL] or [RECEIVE] at the beginning to easily read the logs.

Created the class and passed all info. Issue is getting sockets to connect to each other.

Ok, so it looks like we need to run all the "setup_connection" in a seperate thread to make sure
the new connection can keep listening. These create sockets looking for connection then immediately puts the socket through to
handling requests that listens and adds to the vm's queue.

Another issue (that took way to long to fix) is that we can't create the connections before creating the other machines
(something so obvious now, but it is what it is).
Before running all of the machines, we run a separate function "connect_to_peers" that needs to be run following all initialization.

UNIT TESTING:
- Added tests for connecting sockets
- Added tests for sending information between machines

Ok, great. All seem to be connected. Now, we can think about our main loop.

When running our main loop, we sleep for 1/clock seconds per internal.
Randomize number, if 1 send to first peer, if 2 send to second. If 3rd, iterate and send to others.
If we have an internal event. just update clock by 1.
If we RECEIVED something, use the max formula: clock = max(clock, receive_clock) + 1

We decided that if something is in the queue, the machine forgoes its iternal action to properly respond.
This was so we could better capture drift from slower clock speeds constantly trying to keep up with faster ones.

UNIT TESTING:
- VM can send message to 1 machine
- VM can send messages to both machines
- When received message, clock updates correctly (max function)
- When internal message, clock updates correctly (+1)

Ok, adding all log statements with event types and we should be done.
Next, we can document our results. We've also added a config file for detailing ports, host, and time to run.

RESULTS: We run our scale model five times for a minute each (logs stored under log_hist directory). 
We then have a brief program that extracts clock information from each of the log files
and analyzes the clock information. We report below, for each run, the (randomly generated)
clock rates for each machine (higher numbers indicate more ticks per real world second),
along with the final logical clock time and the average change in the logical clock value
between logged events (which we call "average step size"). 

NORMAL RUNS:
Run 1:
Clock rates: [3, 4, 1]
Final logical clock time: [236, 236, 184]
Jump time: [1.33, 1.0, 3.1]

Run 2:
Clock rates: [5, 3, 6]
Final logical clock time: [351, 349, 351]
Jump time: [1.19, 1.97, 1.0]

Run 3:
Clock rates: [1, 3, 4]
Final logical clock time: [163, 233, 236]
Jump time: [2.75, 1.31, 1.0]

Run 4:
Clock rates: [5, 3, 2]
Final logical clock time: [294, 291, 286]
Jump time: [1.0, 1.64, 2.42]

Run 5:
Clock rates: [6, 2, 4]
Final logical clock time: [351, 334, 348]
Jump time: [1.0, 2.82, 1.48]

LOWER CLOCK SPEED VARIABILITY:
Run 1:
Clock rates: [1, 2, 1]
Max clocks: [118, 120, 113]
Average step sizes: [1.98, 1.0, 1.9]

Run 2:
Clock rates: [2, 1, 1]
Max clocks: [119, 118, 117]
Average step sizes: [1.0, 1.98, 1.97]

Run 3:
Clock rates: [2, 1, 2]
Max clocks: [119, 115, 119]
Average step sizes: [1.0, 1.93, 1.0]

Run 4:
Clock rates: [2, 1, 2]
Max clocks: [119, 116, 119]
Average step sizes: [1.0, 1.95, 1.0]

Run 5:
Clock rates: [2, 2, 2]
Max clocks: [119, 119, 119]
Average step sizes: [1.0, 1.0, 1.0]

LOWER INTERNAL CHANCE:
Run 1:
Clock rates: [1, 4, 6]
Max clocks: [110, 351, 352]
Average step sizes: [1.85, 1.48, 1.0]

Run 2:
Clock rates: [5, 2, 6]
Max clocks: [349, 196, 351]
Average step sizes: [1.19, 1.65, 1.0]

Run 3:
Clock rates: [3, 6, 3]
Max clocks: [330, 351, 348]
Average step sizes: [1.86, 1.0, 1.96]

Run 4:
Clock rates: [4, 1, 3]
Max clocks: [236, 105, 236]
Average step sizes: [1.0, 1.76, 1.33]

Run 5:
Clock rates: [2, 1, 5]
Max clocks: [220, 121, 294]
Average step sizes: [1.86, 2.03, 1.0]

HIGHER INTERNAL CHANCE:
Run 1:
Clock rates: [6, 2, 4]
Max clocks: [352, 352, 352]
Average step sizes: [1.0, 2.95, 1.49]

Run 2:
Clock rates: [1, 2, 3]
Max clocks: [166, 171, 178]
Average step sizes: [2.8, 1.44, 1.0]

Run 3:
Clock rates: [5, 6, 5]
Max clocks: [346, 351, 347]
Average step sizes: [1.18, 1.0, 1.18]

Run 4:
Clock rates: [1, 2, 2]
Max clocks: [115, 119, 119]
Average step sizes: [1.93, 1.0, 1.0]

Run 5:
Clock rates: [1, 6, 1]
Max clocks: [335, 351, 330]
Average step sizes: [5.66, 1.0, 5.58]

We notice a few primary patterns:
(1) Virtual machines with a slower clock rate have greater terminating drift time
(i.e., there is a larger gap between logical clock and wall clock at the end, and 
the final logical clock time is greater). This can be attributed to two primary effects. 
The first (and less significant) effect is due to the increased rate of actions: 
virtual machines with faster clock rates simply execute more actions, 
so it is possible that a few more actions are executed by one machine between 
another machine's last action and the end of the one minute interval. 
However, this effect should not be too pronounced because the number of such actions is highly 
limited because the interval between actions is fairly short.

This limited effect doesn't explain some of the extremely large terminating drift times: 
for instance, in run 3, the final logical clock value of machine 1 is smaller than that of 
machine 2 by almost 100. However, when we analyze the contents of the log file, we see that this 
difference is in large part due to processing speed. Since only one incoming message (which is placed 
in the queue) can be processed during each tick and the the logical clock is only updated as incoming 
messages are processed (since other actions don't occur when there are messages in the inbound
queue), machine 1 effectively falls "behind" on updating its logical clock. In short, the 
number of inbound messages overwhelms the processing speed of this slower machine, which leads
to the observed gap in logical clock times.

However, besides the issue of slowly processing messages (if messages are important, we would
not like them to be in the inbound queue for so long), this gap isn't a logical issue, since
no other internal actions or outbound messages are sent until all inbound messages are
processed (at which point the logical clocks would be more synchronized).

(2) Virtual machines with a slower clock rate have higher jump times. 
This can be attributed to the fact that virtual machines wih higher clock rates simply do
things more quickly. Between ticks of a slower virtual machine, a faster virtual machine may
perform several internal actions and may send/receive messages with a similarly faster
virtual machine. Therefore, while the difference in logged logical clock times between each of
these events on the faster machine may only be one or two, the slower machine might just
process a single message during this time span, making the gap between its last logged action
and this one much greater.

(3) When we lower the clock speed variability (from a random integer between 1 and 6 to a 
random integer between 1 and 2), we notice that the final logical clock times are much
closer together (i.e., differences in terminating drift time are much smaller). This makes
sense because the issues discussed in (1) are largely eliminated. Because clock speeds are
closer together, it's even less likely that a significant number of events are completed
between one machine's last action and another (and this, recall, was already the smaller
effect). Additionally, because clock speeds are closer together, we're less likely to run
into the issue where one machine is overwhelmed by messages from another machine, which
thus decreases the occurences of large differences in terminating drift time.

We should also see that the jump times decrease because the issue mentioned
in (2) is resolved (i.e., the number of actions done by one VM between actions of
another is reduced).

(4) When we lower the probability of internal events per tick, we see increased differences
in terminating drift time and decreased jump times. Lowering the probability of internal events
increases the number of messages / decreases the logical clock gaps between messages. For
the slower VMs that spend their entire lifetime processing messages, this means that
the gap between consecutive messages is smaller, so their logical clock updates are smaller.
At the same time, since the number of messages that the slower VMs can process is about the
same, it means that the overall difference between the logical clock times contained in the 
first and last messages processed is smaller, which leads to a larger difference in
terminating drift time. 

On the other hand, if we increase the probability of internal events per tick
(from 7/10 to 97/100), we see the reverse effect. Differences in the terminating drift time 
decrease significantly and jump times increase. This is largely because the number of 
messages significantly decreases, which largely eliminates the issue of backed up queues. 
However, this effect is also counterbalanced by the fact that there are now less opportunities
for our VMs to synchronize logical clocks via messages, which can lead to
(a) large gaps in terminating drift time if there is a large gap between the last message 
sent between a fast and a slow VM and (b) generally larger jump times.